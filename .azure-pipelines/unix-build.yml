# Variables:
#   CACHE_VERSION: unique cache identifier
#   CURRENT_WEEK: weekly changing cache identifier
#   PYTHON_VERSION: string in the form of "3.x"
#   TODAY_ISO: today's date in ISO format, eg. "20200531"

steps:

# Obtain a shallow clone of the DIALS repository.
# DIALS will not be able to report proper version numbers
- checkout: self
  path: ./modules/dials
  fetchDepth: 1
  displayName: Checkout $(Build.SourceBranch)

# Download source repositories using the bootstrap script
- bash: |
    set -eux
    python modules/dials/installer/bootstrap.py update
  displayName: Repository checkout
  workingDirectory: $(Pipeline.Workspace)

# Create a new conda environment using the bootstrap script
- script: |
    set -eux
    python modules/dials/installer/bootstrap.py base --mamba --clean --python $(PYTHON_VERSION)
  displayName: Create python $(PYTHON_VERSION) environment
  workingDirectory: $(Pipeline.Workspace)

# Copy GL/GLU/KHR headers into a place where they can be found during the
# build. We don't include system headers so that we don't accidentally
# pick up libc and friends
- bash: |
    set -eux
    mkdir -p modules/lib build/include
    cp -av /usr/include/GL build/include
    cp -av /usr/include/KHR build/include
    cp -av /usr/lib/x86_64-linux-gnu/libGL.so* modules/lib
    cp -av /usr/lib/x86_64-linux-gnu/libGLU.so* modules/lib
  displayName: Set up GL/GLU libraries and headers
  workingDirectory: $(Pipeline.Workspace)
  condition: and(succeeded(), eq(variables['Agent.OS'], 'Linux'))

# Build DIALS using the bootstrap script
- bash: |
    set -eux
    python modules/dials/installer/bootstrap.py build
  displayName: DIALS build
  workingDirectory: $(Pipeline.Workspace)

# Ensure we are using up-to-date testing packages.
# Extract the dials-data version so we can correctly cache regression data.

# Recover disk space after testing
# This is only relevant if we had cache misses, as free disk space is required to create cache archives
- bash: |
    set -eux
    echo Disk space usage:
    df -h
    du -sh *
    echo
    echo Test artefacts:
    # du -h tests
    rm -rf tests
  displayName: Recover disk space
  workingDirectory: $(Pipeline.Workspace)
# condition: and(succeeded(), ne(variables.DATA_CACHED, 'true'))

# Expand test environment to allow running xfel tests
- bash: |
    set -e
    . dials
    conda install -p conda_base -y git-lfs distro pandas openmpi mpi4py bash -c conda-forge
    cd modules
    git clone https://gitlab.com/cctbx/xfel_regression.git
    cd xfel_regression
    git lfs install --local
    git lfs pull
    libtbx.configure xfel_regression
  displayName: Prepare xfel_regression tests
  workingDirectory: $(Pipeline.Workspace)

# Finally, run the full regression test suite
- bash: |
    set -e
    . dials
    mkdir tests
    cd tests
    libtbx.run_tests_parallel module=xfel_regression nproc=auto
  displayName: Run tests
  workingDirectory: $(Pipeline.Workspace)

# Recover disk space after testing
# This is only relevant if we had cache misses, as free disk space is required to create cache archives
- bash: |
    set -eux
    echo Disk space usage:
    df -h
    du -sh *
    du -sh modules/xfel_regression
    echo
    echo Test artefacts:
    du -h tests
    rm -rf tests
  displayName: Recover disk space
  workingDirectory: $(Pipeline.Workspace)
# condition: and(succeeded(), ne(variables.DATA_CACHED, 'true'))

